# Fraudulent Transaction Detection Model
Financial transactions labeled as fraudulent or legitimate

## Purpose
The rapid development in the financial industry has exposed significant risks, one of the most critical being the detection of fraudulent activities. This project aims to detect and predict fraudulent transactions by analyzing clients' transaction history, spending patterns, and account behaviors to identify anomalies and prevent financial losses.

## Dataset
- **Link**: [Fraud Transaction Detection Dataset](https://www.kaggle.com/datasets/sanskar457/fraud-transaction-detection/data)
- **Description**: 
  - The dataset includes 1.75 million transactions generated by simulated users over a period from January 2023 to June 2023.
  - The data is highly imbalanced, with only 0.1345% of transactions classified as fraudulent.
  
## Project Overview

The main objective is to analyze and preprocess the data, build a model using machine learning techniques, and evaluate its performance to accurately predict the likelihood of credit default.

### 1. Data Analysis
- **Imbalance Issue**: 
  - The dataset contains an imbalanced class distribution, with very few fraudulent transactions compared to non-fraudulent ones. This imbalance could lead to biased model performance.
  
### 2. Data Preprocessing
To address the imbalance, we will apply **Synthetic Minority Over-sampling Technique (SMOTE)**. SMOTE generates synthetic samples for the minority class (fraudulent transactions) to balance the dataset and improve the model's ability to detect fraud.

However, before applying SMOTE, we split the data to prevent oversampling from affecting the validation dataset, ensuring the model is evaluated on real, unaltered data.

#### Steps:
1. **Train-test Split**:
   - The data is first split into training and testing sets using an 80/20 ratio, preserving the test set as a benchmark for model evaluation. Since the dataset is highly imbalanced, we apply **SMOTE** (Synthetic Minority Over-sampling Technique) to the training set only. This ensures the model is trained on balanced data while the test set remains untouched, preserving its original distribution for unbiased evaluation.

2. **Preprocessing**:
   - **Min-Max Scaling**: We apply `MinMaxScaler()` to scale all numeric features (e.g., transaction amount) between 0 and 1. This ensures that features with larger ranges don't dominate the model's learning process.
   - **Categorical Encoding**: Categorical features (e.g., customer ID, terminal ID) are converted into numerical format using `OneHotEncoder(handle_unknown='ignore')`, which creates binary columns for each category. This allows the model to interpret categorical variables effectively without assuming any ordinal relationship.

